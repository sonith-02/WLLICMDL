{"cells":[{"cell_type":"markdown","metadata":{"id":"fvhOkmrVA7d8"},"source":["### ***ALL IN ONE ***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KV9ycGp5DOew"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","from tqdm import tqdm\n","\n","# Load and preprocess data\n","def load_and_prepare_data(train_path, val_path):\n","    train_df = pd.read_csv(train_path).dropna()\n","    validation_df = pd.read_csv(val_path).dropna()\n","\n","    train_texts, train_labels = train_df['Word'].tolist(), train_df['Tag'].tolist()\n","    validation_texts, validation_labels = validation_df['Word'].tolist(), validation_df['Tag'].tolist()\n","\n","    label_encoder = LabelEncoder()\n","    train_labels_encoded = label_encoder.fit_transform(train_labels)\n","    validation_labels_encoded = label_encoder.transform(validation_labels)\n","\n","    return train_texts, train_labels_encoded, validation_texts, validation_labels_encoded, label_encoder\n","\n","# Custom dataset creation\n","def create_dataset(texts, labels, tokenizer, max_length):\n","    input_ids_list = []\n","    attention_mask_list = []\n","    labels_list = []\n","\n","    for text, label in zip(texts, labels):\n","        encoding = tokenizer(\n","            str(text),\n","            truncation=True,\n","            padding='max_length',\n","            max_length=max_length,\n","            return_tensors='pt'\n","        )\n","        input_ids_list.append(encoding['input_ids'].squeeze())\n","        attention_mask_list.append(encoding['attention_mask'].squeeze())\n","        labels_list.append(torch.tensor(label, dtype=torch.long))\n","\n","    dataset = list(zip(input_ids_list, attention_mask_list, labels_list))\n","    return dataset\n","\n","# Training and evaluation function\n","def train_and_evaluate(train_texts, train_labels_encoded, validation_texts, validation_labels_encoded, label_encoder, batch_size=8, max_length=128, num_epochs=3):\n","    bert_model_name = 'bert-base-multilingual-cased'\n","    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n","    model = BertForSequenceClassification.from_pretrained(bert_model_name, num_labels=len(label_encoder.classes_))\n","\n","    train_dataset = create_dataset(train_texts, train_labels_encoded, tokenizer, max_length)\n","    validation_dataset = create_dataset(validation_texts, validation_labels_encoded, tokenizer, max_length)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = AdamW(model.parameters(), lr=2e-5)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    print(\"Start training\")\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","        correct_train = 0\n","        total_train = 0\n","\n","        with tqdm(train_loader, unit=\"batch\") as t:\n","            for batch in t:\n","                input_ids, attention_mask, labels = batch\n","                input_ids = input_ids.to(device)\n","                attention_mask = attention_mask.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = outputs.loss\n","                logits = outputs.logits\n","\n","                loss.backward()\n","                nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n","                optimizer.step()\n","\n","                total_loss += loss.item()\n","                _, predicted = torch.max(logits, 1)\n","                total_train += labels.size(0)\n","                correct_train += (predicted == labels).sum().item()\n","\n","                t.set_postfix({'loss': total_loss / (t.n + 1), 'accuracy': correct_train / total_train})\n","\n","    model.eval()\n","    y_true, y_pred = [], []\n","\n","    with torch.no_grad():\n","        with tqdm(validation_loader, unit=\"batch\") as t:\n","            for batch in t:\n","                input_ids, attention_mask, labels = batch\n","                input_ids = input_ids.to(device)\n","                attention_mask = attention_mask.to(device)\n","                labels = labels.to(device)\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","                logits = outputs.logits\n","                _, predicted = torch.max(logits, 1)\n","\n","                y_true.extend(labels.cpu().numpy())\n","                y_pred.extend(predicted.cpu().numpy())\n","\n","    # Metrics\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","\n","    # Return y_true, y_pred, and label encoder for the combined report\n","    return y_true, y_pred, label_encoder\n","\n","\n","# File paths for datasets\n","datasets = {\n","    \"Tamil\": {\n","        \"train\": \"/content/drive/MyDrive/DATASET/all language correct format dataset /correct_tamil_dataset.csv\",\n","        \"validation\": \"/content/drive/MyDrive/DATASET/all language correct format dataset /correct_tamil_validation\"\n","    },\n","    \"Malayalam\": {\n","        \"train\": \"/content/drive/MyDrive/DATASET/all language correct format dataset /Final_mal_train(80%)  (1).csv\",\n","        \"validation\": \"/content/drive/MyDrive/DATASET/all language correct format dataset /Final_mal_dev(20%) (1).csv\"\n","    },\n","    \"Tulu\": {\n","        \"train\": \"/content/drive/MyDrive/DATASET/all language correct format dataset /correct_tulu_train_set\",\n","        \"validation\": \"/content/drive/MyDrive/DATASET/all language correct format dataset/correct_tulu_validation_set\"\n","    },\n","    \"Kannada\": {\n","        \"train\": \"/content/drive/MyDrive/DATASET/all language correct format dataset/correct_kannada_train\",\n","        \"validation\": \"/content/drive/MyDrive/DATASET/all language correct format dataset /correct_tulu_validation_set\"\n","    }\n","}\n","\n","# Combined classification report storage\n","combined_y_true = []\n","combined_y_pred = []\n","combined_label_encoders = []\n","\n","# Evaluate the model on different datasets and accumulate results\n","for language, paths in datasets.items():\n","    print(f\"Evaluating for {language}\")\n","    train_texts, train_labels_encoded, validation_texts, validation_labels_encoded, label_encoder = load_and_prepare_data(paths['train'], paths['validation'])\n","    y_true, y_pred, label_encoder = train_and_evaluate(train_texts, train_labels_encoded, validation_texts, validation_labels_encoded, label_encoder)\n","\n","    combined_y_true.extend(y_true)\n","    combined_y_pred.extend(y_pred)\n","    combined_label_encoders.append(label_encoder)\n","\n","# Print combined classification report\n","# Assuming all label encoders have the same labels (you should ensure this is correct)\n","if len(combined_label_encoders) > 0:\n","    common_labels = combined_label_encoders[0].classes_  # Assuming all datasets have the same labels\n","    print(\"\\nCombined Classification Report:\")\n","    print(classification_report(combined_y_true, combined_y_pred, target_names=common_labels))\n"]},{"cell_type":"code","source":[],"metadata":{"id":"ui6JLSENWlLv"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"18q0T__cU2-FICR7-ZHYAQITQrv8jdRI0","timestamp":1723471704595}],"authorship_tag":"ABX9TyO/sxB/1goNJxuDVOT4JaTu"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}