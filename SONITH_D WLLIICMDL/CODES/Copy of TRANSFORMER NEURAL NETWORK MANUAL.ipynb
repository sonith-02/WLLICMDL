{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1on5noCFk_qSr2noicAKoewyNSAKliRXW","timestamp":1723471405291}],"gpuType":"T4","authorship_tag":"ABX9TyPQZ8sTx9hAuXwF4FGar9xP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### ***ALL IN ONE ***"],"metadata":{"id":"ciEF5itWHib-"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Embedding, LayerNormalization, Dropout, MultiHeadAttention\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score"],"metadata":{"id":"zDiafIkPHlCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load and preprocess data\n","def load_and_preprocess_data(train_filepath, val_filepath):\n","    train_df = pd.read_csv(train_filepath).dropna()\n","    val_df = pd.read_csv(val_filepath).dropna()\n","    X_train, y_train = train_df['Word'], train_df['Tag']\n","    X_test, y_test = val_df['Word'], val_df['Tag']\n","    return X_train, y_train, X_test, y_test\n","\n","# Define parameters\n","MAX_SEQUENCE_LENGTH = 128\n","EMBEDDING_DIM = 100"],"metadata":{"id":"wyWbHLK_HlRV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Define function for creating and compiling the model\n","def create_and_compile_model(num_tags, vocab_size, embed_dim=EMBEDDING_DIM, num_heads=4, ff_dim=64):\n","    def transformer_block(inputs, embed_dim, num_heads, ff_dim, rate=0.1, training=False):\n","        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n","        attn_output = Dropout(rate)(attn_output, training=training)\n","        out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n","        ffn_output = Dense(ff_dim, activation=\"relu\")(out1)\n","        ffn_output = Dense(embed_dim)(ffn_output)\n","        ffn_output = Dropout(rate)(ffn_output, training=training)\n","        return LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n","\n","    def token_and_position_embedding(inputs, maxlen, vocab_size, embed_dim):\n","        token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n","        pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)(tf.range(start=0, limit=maxlen, delta=1))\n","        return token_emb + pos_emb\n","\n","    inputs = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,))\n","    x = token_and_position_embedding(inputs, MAX_SEQUENCE_LENGTH, vocab_size, embed_dim)\n","    x = transformer_block(x, embed_dim, num_heads, ff_dim)\n","    x = Dropout(0.1)(x)\n","    x = Dense(ff_dim, activation=\"relu\")(x)\n","    x = Dropout(0.1)(x)\n","    outputs = Dense(num_tags, activation=\"softmax\")(x)\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs)"],"metadata":{"id":"BDa2mkGWHlOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    # Define custom loss function\n","    class CustomNonPaddingTokenLoss(tf.keras.losses.Loss):\n","        def __init__(self, name=\"custom_ner_loss\"):\n","            super().__init__(name=name)\n","        def call(self, y_true, y_pred):\n","            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n","            loss = loss_fn(y_true, y_pred)\n","            mask = tf.cast((y_true > 0), dtype=tf.float32)\n","            loss = loss * mask\n","            return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n","\n","    def acc(y_true, y_pred):\n","        targ = tf.cast(y_true, dtype='int32')\n","        pred = tf.cast(tf.argmax(y_pred, axis=-1), dtype='int32')\n","        correct = tf.cast(tf.equal(targ, pred), dtype='float32')\n","        mask = tf.cast(tf.greater(targ, 0), dtype='float32')\n","        n_correct = tf.reduce_sum(mask * correct)\n","        n_total = tf.reduce_sum(mask)\n","        return n_correct / n_total\n","\n","    model.compile(optimizer='adam', loss=CustomNonPaddingTokenLoss(), metrics=[acc])\n","    return model"],"metadata":{"id":"GvDID7FlHlMY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# File paths for different datasets\n","datasets = {\n","    \"Tamil\": {\n","        \"train\": \"/content/drive/MyDrive/seq2seq/TAMIL DATASET/tamil_train (1).csv\",\n","        \"validation\": \"/content/drive/MyDrive/seq2seq/TAMIL DATASET/tamil_val.csv\"\n","    },\n","     \"Malayalam\": {\n","        \"train\": \"/content/drive/MyDrive/seq2seq/MALAYALAM DATASET/Final_mal_train(80%).csv\",\n","        \"validation\": \"/content/drive/MyDrive/seq2seq/MALAYALAM DATASET/Final_mal_dev(20%).csv\"\n","\n","    },\n","    \"Tulu\": {\n","        \"train\": \"/content/drive/MyDrive/seq2seq/TULU DATASET/tulu_train (1) (1).csv\",\n","         \"validation\":\"/content/drive/MyDrive/seq2seq/TULU DATASET/tulu_val (1).csv\"\n","    },\n","   \"Kannada\": {\n","        \"train\": \"/content/drive/MyDrive/seq2seq/KANNADA DATASET/kn_train (1) (1).csv\",\n","        \"validation\": \"/content/drive/MyDrive/seq2seq/KANNADA DATASET/kn_val (1).csv\"\n","\n","    }\n","}"],"metadata":{"id":"UU1W-qFQHlJp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate model on each dataset\n","for language, paths in datasets.items():\n","    print(f\"\\nProcessing {language} dataset...\")\n","\n","    X_train, y_train, X_test, y_test = load_and_preprocess_data(paths['train'], paths['val'])\n","\n","    # Parameters\n","    Vx = 20000  # You may want to adjust this parameter based on your vocabulary size\n","    Vy = len(y_train.unique())\n","\n","    # Text Tokenization\n","    text_tokenizer = Tokenizer(num_words=Vx, oov_token='<OOV>', filters='')\n","    text_tokenizer.fit_on_texts(X_train)\n","    text_sequences_train = text_tokenizer.texts_to_sequences(X_train)\n","    text_sequences_test = text_tokenizer.texts_to_sequences(X_test)\n","\n","    # Label Tokenization\n","    li_tokenizer = Tokenizer(num_words=Vy+1, filters='', oov_token='<OOV>')\n","    li_tokenizer.fit_on_texts(y_train)\n","    li_sequences_train = li_tokenizer.texts_to_sequences(y_train)\n","    li_sequences_test = li_tokenizer.texts_to_sequences(y_test)\n","\n","    # Padding\n","    text_inputs_train = pad_sequences(text_sequences_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n","    text_inputs_test = pad_sequences(text_sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n","    li_targets_train = pad_sequences(li_sequences_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n","    li_targets_test = pad_sequences(li_sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n","\n","    # Create and compile model\n","    model = create_and_compile_model(num_tags=Vy + 1, vocab_size=Vx)\n","\n","    # Train model with Early Stopping\n","    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","    history = model.fit(text_inputs_train, li_targets_train, batch_size=32, epochs=15, validation_split=0.2, callbacks=[early_stopping])"],"metadata":{"id":"g6i0JgOHHlHD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["   # Save the trained model\n","    model_path = f'/content/drive/MyDrive/seq2seq/{language.lower()}_transformer_model.h5'\n","    model.save(model_path)\n","\n","    # Evaluate model\n","    loss, accuracy = model.evaluate(text_inputs_test, li_targets_test)\n","    print(f\"Evaluation Results for {language}:\")\n","    print(f\"Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n","\n","    # Predict and convert predictions to labels\n","    y_pred_prob = model.predict(text_inputs_test)\n","    y_pred = np.argmax(y_pred_prob, axis=-1)\n","\n","    # Flatten the arrays for computing the metrics\n","    y_test_flat = li_targets_test.flatten()\n","    y_pred_flat = y_pred.flatten()\n","\n","    # Remove padding from the flattened arrays\n","    non_zero_indices = y_test_flat != 0\n","    y_test_flat_non_zero = y_test_flat[non_zero_indices]\n","    y_pred_flat_non_zero = y_pred_flat[non_zero_indices]\n","\n","    # Compute classification report\n","    print(f\"Classification Report for {language}:\\n\", classification_report(y_test_flat_non_zero, y_pred_flat_non_zero))\n","    print(\"Accuracy:\", accuracy_score(y_test_flat_non_zero, y_pred_flat_non_zero))\n","    print(\"Macro F1 Score:\", f1_score(y_test_flat_non_zero, y_pred_flat_non_zero, average='macro'))\n","    print(\"Precision:\", precision_score(y_test_flat_non_zero, y_pred_flat_non_zero, average='macro'))\n","    print(\"Recall:\", recall_score(y_test_flat_non_zero, y_pred_flat_non_zero, average='macro'))"],"metadata":{"id":"lrJ4DcQpHlEl"},"execution_count":null,"outputs":[]}]}